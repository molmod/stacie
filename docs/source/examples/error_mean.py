#!/usr/bin/env python3

# %% [markdown]
# # Standard Error of the Mean
#
# This notebook shows how to use Stacie to compute the error of the mean
# of a time-correlated input sequence,
# i.e. of which not all values are statistically independent.
#
# This is a completely self-contained example that generates the input sequences
# (with MCMC) and then analyzes them with Stacie.
# Atomic units are used unless otherwise noted.
#
# We suggest you experiment with this notebook by making the following changes:
#
# - Change the number of sequences and their length.
# - Change the correlation time through `PROPOSAL_STEP`.
# - Change the cutoff criterion to the AIC.
#   (Add the argument `cutoff_criterion=akaike_criterion` to `estimate_acint`,
#    where `akaike_criterion` is imported from `stacie.cutoff`.)

# %% [markdown]
# ## Import Libraries and Configure `matplotlib`

# %%
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from scipy.integrate import quad
from stacie import UnitConfig, compute_spectrum, estimate_acint
from stacie.plot import (
    plot_criterion,
    plot_fitted_spectrum,
    plot_residuals,
    plot_spectrum,
    plot_uncertainty,
)

# %%
mpl.rc_file("matplotlibrc")
# %config InlineBackend.figure_formats = ["svg"]

# %% [markdown]
# ## Data Generation
#
# The data for the analysis are generated by sampling a Kratzer--Feus potential
# of a diatomic molecule {cite:p}`kratzer_1920_die,fues_1926_das`
# at constant temperature.
# This potential is harmonic in $1/r$:
#
# $$
#     U(r) = \frac{K}{2} \left(\frac{r_0^2}{r} - r_0\right)^2
# $$
#
# where $K$ is the force constant and $r_0$ the equilibrium bond length.
# The sampled probability density is a Boltzmann distribution:
#
# $$
#     p_r(r) = \frac{1}{Z} \exp\left(-\frac{U(r)}{k_\text{B}T}\right)
# $$
#
# where the normalization $Z$ is the classical partition function.
#
# In this example,
# the force constant and bond length of the lithium dimer are used,
# with parameters from {cite:t}`zhao_2022_bond` converted to atomic units.
# A high temperature is used to skew the distribution to larger distances.

# %%
K = 0.015
R0 = 5.150
TEMPERATURE = 1000
BOLTZMANN = 3.167e-6
BETA = 1 / (BOLTZMANN * TEMPERATURE)
PROPOSAL_STEP = 0.1


def logprob(r):
    """Calculate the logarithm of the probability."""
    energy = 0.5 * K * (R0**2 / r - R0) ** 2
    return -BETA * energy


def plot_potential_dist():
    fig, (ax1, ax2) = plt.subplots(2, 1)
    rgrid = np.linspace(0.5 * R0, 2 * R0, 100)
    ax2.sharex(ax1)
    ax1.plot(rgrid, -logprob(rgrid) / BETA)
    ax1.set_ylabel(r"Potential energy [E$_\text{h}$]")
    ax2.plot(rgrid, np.exp(logprob(rgrid)))
    ax2.set_ylabel("Boltzmann factor\n(unnormalized probability density)")
    ax2.set_xlabel("Internuclear distance [a$_0$]")


plot_potential_dist()


# %% [markdown]
# The MCMC implementation below is non-standard in the sense that it
# is vectorized to generate multiple sequences in parallel.


# %%
def sample_mcmc_chain(niter, stride, ndim, burnin, seed=42):
    """Sample independent Markov Chains with the Metropolis algorithm."""
    rng = np.random.default_rng(seed)
    result = np.zeros((ndim, niter // stride))
    r_old = np.full(ndim, R0)
    lp_old = logprob(r_old)
    irow = 0
    istep = 0
    while irow < result.shape[1]:
        r_new = r_old + rng.normal(0, PROPOSAL_STEP, ndim)
        lp_new = logprob(r_new)
        accept = lp_new > lp_old
        mask = ~accept
        nrnd = mask.sum()
        if nrnd > 0:
            accept[mask] = rng.uniform(0, 1, nrnd) < np.exp(lp_new[mask] - lp_old[mask])
        r_old[accept] = r_new[accept]
        lp_old[accept] = lp_new[accept]
        if burnin > 0:
            burnin -= 1
            continue
        if istep % stride == 0:
            result[:, irow] = r_new
            irow += 1
        istep += 1
    return result


sequences = sample_mcmc_chain(20000, 5, 50, 200)
print(f"(nseq, nstep) = {sequences.shape}")
mean_mc = sequences.mean()
print(f"Monte Carlo E[r] ≈ {mean_mc:.5f} > {R0:.5f}")

# %% [markdown]
# Because of the finite temperature and the anharmonicity of the potential,
# the average distance is greater than the equilibrium bond length.


# %%
# Plot the beginning of a few sequences.
# The atomic unit of length is the Bohr radius, $\mathrm{a}_0$.
def plot_chains():
    fig, ax = plt.subplots()
    ax.plot(sequences[0][:500])
    ax.plot(sequences[1][:500])
    ax.plot(sequences[2][:500])
    ax.set_xlabel("Step")
    ax.set_ylabel(r"Bond length [a$_0$]")
    ax.set_title("Markov Chain samples")


plot_chains()

# %% [markdown]
#
# The sequences in the plot are clearly time-correlated.
# The following cells show how Stacie can be used
# to compute the uncertainty of this average,
# taking into account that not all samples
# are independent due to time correlations.

# %% [markdown]
# ## Uncertainty Quantification
#
# The spectrum is calculated with settings appropriate for error estimation.
# See the [Error Estimates](../theory/properties/error_estimates.md) section
# for more details.

# %%
# Compute and plot the power spectrum.
spectrum = compute_spectrum(
    sequences,
    prefactor=1 / sequences.size,
    include_zero_freq=False,
)

# %% [markdown]
# The `UnitConfig` object contains settings
# that are reused by most plotting functions.
# The integral has units of length squared, $\mathrm{a}_0^2$.
# (It is the variance of the mean.)
# In MC sampling,
# time and frequency are fictitious and therefore made dimensionless here.

# %%
uc = UnitConfig(
    acint_fmt=".1e",
    acint_unit_str=r"a$^2_0$",
    freq_unit_str="1",
    time_unit_str="1",
)
fig, ax = plt.subplots()
plot_spectrum(ax, uc, spectrum, 180)

# %% [markdown]
# From the spectrum, one can already visually estimate the variance of the mean:
# the limit to zero frequency is about $3.5 \times 10^{-5}\,\mathrm{a}_0^2$.
# By normalizing the spectrum with the total simulation time,
# the spectrum has a unit of length squared,
# which is correct for the variance in this case.
# In the following,
# a model is fitted to the spectrum to get a more precise estimate.

# %%
result = estimate_acint(spectrum, verbose=True)

# %%
# The essential result:
error_mc = np.sqrt(result.acint)
print(f"Error of the mean = {error_mc:.5f}")

# Because Stacie can estimate errors of the autocorrelation integral,
# it can also estimate errors of errors of means.
error_of_error_mc = 0.5 * result.acint_std / error_mc
print(f"Uncertainty of the error of mean = {error_of_error_mc:.5f}")

# %%
# Plot of the empirical and fitted model spectrum.
fig, ax = plt.subplots()
plot_fitted_spectrum(ax, uc, result)

# %%
# Plot of the underfitting criterion minimization
# as a function of the frequency cutoff.
fig, ax = plt.subplots()
plot_criterion(ax, uc, result)

# %%
# Plot of the normalized residuals.
fig, ax = plt.subplots()
plot_residuals(ax, uc, result)

# %%
# Plot of the error of the mean and its uncertainty
# as a function of the frequency cutoff.
fig, ax = plt.subplots()
plot_uncertainty(ax, uc, result)

# %% [markdown]
# ## Precise Mean With Numerical Quadrature
#
# Because the probability density is one-dimensional,
# it is feasible to compute the mean using numerical quadrature,
# which is much more accurate than the Monte Carlo estimate.
# (For production simulations,
# Monte Carlo is only advantageous for high-dimensional problems.)
#
# As shown in the code below,
# the difference between the quadrature and Monte Carlo estimates
# is on the order of the estimated uncertainty of the MC result.

# %%
numer_quad = quad(lambda r: r * np.exp(logprob(r)), 0, 50)[0]
denom_quad = quad(lambda r: np.exp(logprob(r)), 0, 50)[0]
mean_quad = numer_quad / denom_quad
print(f"Quadrature  E[r]   ≈ {mean_quad:8.5f}")
print(f"Monte Carlo E[r]   ≈ {mean_mc:8.5f}")
print(f"|Difference|       = {abs(mean_quad - mean_mc):8.5f}")
print(f"Estimated MC error = {error_mc:8.5f}")

# %% [markdown]
# ## Autocorrelation time
#
# The Exponential Tail model directly estimates the correlation time from the width
# of the peak at zero frequency in the spectrum.
# This correlation time is formally equivalent to the "exponential autocorrelation time",
# as defined by Sokal {cite:p}`sokal_1997_monte`.
# It may differ from the "integrated autocorrelation time".
# Only if the autocorrelation function is nothing but an exponentially decaying function,
# both should match.

# %%
print("Autocorrelation times:")
print(f"exponential: {result.props['corrtime_exp']:.2f}")
print(f"integrated: {result.corrtime_int:.2f}")

# %% [markdown]
# In this case, both autocorrelation times agree quite well.

# %%  [markdown]
# ## Regression Tests
#
# If you are experimenting with this notebook, you can ignore any exceptions below.
# The tests are only meant to pass for the notebook in its original form.

# %%
if abs(mean_mc - 5.28807) > 1e-3:
    raise ValueError(f"Wrong mean_mc: {mean_mc:.5f}")
if abs(error_mc - 0.00577) > 1e-3:
    raise ValueError(f"Wrong error_mc: {error_mc:.5f}")
