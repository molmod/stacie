#!/usr/bin/env python

# %% [markdown]
# # Error on the Mean
#
# This notebook illustrates how to use Stacie to compute an error on the mean
# of a time-correlated input sequence.
#
# This is a fully self-contained example that generates the input sequences (with MCMC)
# and then analyses them with Stacie.

# %% [markdown]
# ## Import Libraries and Configure `matplotlib`

# %%
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from scipy.integrate import quad
from stacie import UnitConfig, estimate_acfint, prepare_acfint
from stacie.plot import plot_fitted_spectrum, plot_risk, plot_spectrum, plot_uncertainty

# %%
mpl.rc_file("matplotlibrc")
# %config InlineBackend.figure_formats = ["svg"]

# %% [markdown]
# ## Data Generation
#
# The data for the analysis are generated by Sampling a 1D Kratzer-Feus potential
# of a diatomic molecule at a constant temperature.
# This potential is harmonic in $1/r$:
#
# $$
#     U(r) = \frac{K}{2} \left(\frac{r_0^2}{r} - r_0\right)^2
# $$
#
# where $K$ is the force constant and $r_0$ the equilibrium bond length.
# The sampled probability density is a Boltzmann distribution with an inverse temperature $\beta$.
#
# As a result of the finite temperature,
# the average distance will be larger than the bond length at rest.
#
# For simplicity, this example works in arbitrary units and uses round parameter values.
#
# The MCMC implementation below is non-standard in the sense that it
# is vectorized to generate multiple sequences in parallel.

# %%
K = 0.4
R0 = 1.5
BETA = 50.0


def logprob(r):
    """Calculate the logarithm of the probability."""
    energy = 0.5 * K * (R0**2 / r - R0) ** 2
    return -BETA * energy


def sample_mcmc_chain(niter, stride, ndim, burnin, seed=42):
    """Sample independent Metroplis Markov Chains with the Metropolis Monte Carlo algorithm."""
    rng = np.random.default_rng(seed)
    result = np.zeros((ndim, niter // stride))
    r_old = np.full(ndim, R0)
    lp_old = logprob(r_old)
    irow = 0
    istep = 0
    while irow < result.shape[1]:
        r_new = r_old + rng.normal(0, 0.05, ndim)
        lp_new = logprob(r_new)
        accept = lp_new > lp_old
        mask = ~accept
        nrnd = mask.sum()
        if nrnd > 0:
            accept[mask] = rng.uniform(0, 1, nrnd) < np.exp(lp_new[mask] - lp_old[mask])
        r_old[accept] = r_new[accept]
        lp_old[accept] = lp_new[accept]
        if burnin > 0:
            burnin -= 1
            continue
        if istep % stride == 0:
            result[:, irow] = r_new
            irow += 1
        istep += 1
    return result


sequences = sample_mcmc_chain(20000, 5, 50, 200)
print(f"(nseq, nstep) = {sequences.shape}")
mean_mc = sequences.mean()
print(f"Monte Carlo E[r] ≈ {mean_mc:.5f}")

# %%
# Plot a few short examples of the chain

fig, ax = plt.subplots()
ax.plot(sequences[0][:500])
ax.plot(sequences[1][:500])
ax.plot(sequences[2][:500])
ax.set_xlabel("Step")
ax.set_ylabel("Bond length")
ax.set_title("Markov Chain samples")

# %% [markdown]
#
# The sequences in the plot are clearly time-correlated.
# The bond length distribution is skewed towards larger values
# due to the anharmonicity (in $r$) of the Kratzer-Feus potential.
# As a result the average bond length is also larger then the equilibrium value.
# In the following cells, it will be shown how Stacie can be used
# to compute the uncertainty on this average,
# taking into account that not all samples are independent due to time correlations.

# %% [markdown]
# ## Uncertainty Quantification
#
# The spectrum is calculated with settings suitable for error estimation.
# For more details, see the section on [Error Estimates](../properties/error_estimates.md).

# %%
# Compute and plot the power spectrum.
spectrum = prepare_acfint(sequences, prefactor=1 / sequences.size, include_zero_freq=False)

# The UnitConfig object contains settings that are reused by most plotting functions.
# The integral has a unit of variance.
# In MC sampling, time and frequency are fictitious and therefore made dimensionless here
uc = UnitConfig(acfint_unit_str="l^2", freq_unit_str="1", time_unit_str="1", acfint_fmt=".1e")
fig, ax = plt.subplots()
plot_spectrum(ax, uc, spectrum, 180)

# %% [markdown]
# From the spectrum, one can already visually estimate the variance on the mean,
# roughly about $2.0 \times 10^{-5}$.
# By normalizing the spectrum with the total simulation time,
# the unit of the spectrum is length squared, which is correct for the variance in this case.
# In the following, a model will be fitted to the spectrum to make a more precise estimate.

# %%
result = estimate_acfint(spectrum, verbose=True)

# %%
# The essential result
error = np.sqrt(result.props["acfint"])
print(f"Error of the mean = {np.sqrt(result.props['acfint']):.5f}")

# Because Stacie can estimate errors on the autocorrelation integral,
# it can also estimate errors of errors of means.
print(f"Uncertainty of the error of mean = {0.5 * result.props['acfint_std'] / error:.5f}")

# %%
# Plot of the empirical and fitted model spectrum
fig, ax = plt.subplots()
plot_fitted_spectrum(ax, uc, result)

# %%
# Plot of the risk minimization as a function of the frequency cutoff
fig, ax = plt.subplots()
plot_risk(ax, uc, result)

# %%
# Plot of the mean and uncertainty as a function of the frequency cutoff
fig, ax = plt.subplots()
plot_uncertainty(ax, uc, result)

# %% [markdown]
## Precise Mean With Numerical Quadrature
#
# Because the probability density is one-dimensional,
# it is feasible to compute the mean with numerical quadrature,
# which will be much more accurate than the Monte Carlo estimate.
# (For production simulations, Monte Carlo is only advantageous for high-dimensional problems.)
#
# As shown in the code below, the difference between the quadrature and Monte Carlo estimates
# is of the order of the estimated uncertainty on the MC result.

# %%
numer_quad = quad(lambda r: r * np.exp(logprob(r)), 0, 1000)[0]
denom_quad = quad(lambda r: np.exp(logprob(r)), 0, 1000)[0]
mean_quad = numer_quad / denom_quad
print(f"Quadrature  E[r]   ≈ {mean_quad:8.5f}")
print(f"Monte Carlo E[r]   ≈ {mean_mc:8.5f}")
print(f"|Difference|       = {abs(mean_quad - mean_mc):8.5f}")
print(f"Estimated MC error = {error:8.5f}")
