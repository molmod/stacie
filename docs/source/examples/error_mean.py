#!/usr/bin/env python

# %% [markdown]
# # Standard Error of the Mean
#
# This notebook shows how to use Stacie to compute the error of the mean
# of a time-correlated input sequence, i.e. of which not all values are statistically independent.
#
# This is a completely self-contained example that generates the input sequences (with MCMC)
# and then analyzes them with Stacie.

# %% [markdown]
# ## Import Libraries and Configure `matplotlib`

# %%
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from scipy.integrate import quad
from stacie import UnitConfig, estimate_acfint, prepare_acfint
from stacie.plot import plot_fitted_spectrum, plot_risk, plot_spectrum, plot_uncertainty

# %%
mpl.rc_file("matplotlibrc")
# %config InlineBackend.figure_formats = ["svg"]

# %% [markdown]
# ## Data Generation
#
# The data for the analysis are generated by sampling a 1D Kratzer-Feus potential
# of a diatomic molecule at constant temperature.
# This potential is harmonic in $1/r$:
#
# $$
#     U(r) = \frac{K}{2} \left(\frac{r_0^2}{r} - r_0\right)^2
# $$
#
# where $K$ is the force constant and $r_0$ the equilibrium bond length.
# The sampled probability density is a Boltzmann distribution with inverse temperature $\beta$.
#
# Because of the finite temperature,
# the average distance will be greater than the equilibrium bond length.
#
# For simplicity, this example works in arbitrary units and uses round parameter values.
#
# The MCMC implementation below is non-standard in the sense that it
# is vectorized to generate multiple sequences in parallel.

# %%
K = 0.4
R0 = 1.5
BETA = 50.0


def logprob(r):
    """Calculate the logarithm of the probability."""
    energy = 0.5 * K * (R0**2 / r - R0) ** 2
    return -BETA * energy


def sample_mcmc_chain(niter, stride, ndim, burnin, seed=42):
    """Sample independent Metroplis Markov Chains with the Metropolis Monte Carlo algorithm."""
    rng = np.random.default_rng(seed)
    result = np.zeros((ndim, niter // stride))
    r_old = np.full(ndim, R0)
    lp_old = logprob(r_old)
    irow = 0
    istep = 0
    while irow < result.shape[1]:
        r_new = r_old + rng.normal(0, 0.05, ndim)
        lp_new = logprob(r_new)
        accept = lp_new > lp_old
        mask = ~accept
        nrnd = mask.sum()
        if nrnd > 0:
            accept[mask] = rng.uniform(0, 1, nrnd) < np.exp(lp_new[mask] - lp_old[mask])
        r_old[accept] = r_new[accept]
        lp_old[accept] = lp_new[accept]
        if burnin > 0:
            burnin -= 1
            continue
        if istep % stride == 0:
            result[:, irow] = r_new
            irow += 1
        istep += 1
    return result


sequences = sample_mcmc_chain(20000, 5, 50, 200)
print(f"(nseq, nstep) = {sequences.shape}")
mean_mc = sequences.mean()
print(f"Monte Carlo E[r] ≈ {mean_mc:.5f}")

# %%
# Plot the beginning of a few sequences.
# The arbitrary unit of length is represented by $\ell$.
fig, ax = plt.subplots()
ax.plot(sequences[0][:500])
ax.plot(sequences[1][:500])
ax.plot(sequences[2][:500])
ax.set_xlabel("Step")
ax.set_ylabel(r"Bond length [$\ell$]")
ax.set_title("Markov Chain samples")

# %% [markdown]
#
# The sequences in the plot are clearly time-correlated.
# The bond length distribution is skewed to larger values
# due to the anharmonicity (in $r$) of the Kratzer-Feus potential.
# As a result, the average bond length is also larger then the equilibrium value.
# The following cells show how Stacie can be used to compute the uncertainty of this average,
# taking into account that not all samples are independent due to time correlations.

# %% [markdown]
# ## Uncertainty Quantification
#
# The spectrum is calculated with settings appropriate for error estimation.
# See the [Error Estimates](../properties/error_estimates.md) section for more details.

# %%
# Compute and plot the power spectrum.
spectrum = prepare_acfint(sequences, prefactor=1 / sequences.size, include_zero_freq=False)

# The UnitConfig object contains settings that are reused by most plotting functions.
# The integral has units of length squared, $\ell^2$. (It is the variance of the mean.)
# In MC sampling, time and frequency are fictitious and therefore made dimensionless here.
uc = UnitConfig(acfint_unit_str=r"$\ell^2$", freq_unit_str="1", time_unit_str="1", acfint_fmt=".1e")
fig, ax = plt.subplots()
plot_spectrum(ax, uc, spectrum, 180)

# %% [markdown]
# From the spectrum, one can already visually estimate the variance of the mean,
# roughly about $2.0 \times 10^{-5}\,\ell^2$.
# By normalizing the spectrum with the total simulation time,
# the spectrum has a unit of length squared, which is correct for the variance in this case.
# In the following, a model is fitted to the spectrum to get a more precise estimate.

# %%
result = estimate_acfint(spectrum, verbose=True)

# %%
# The essential result:
error_mc = np.sqrt(result.props["acfint"])
print(f"Error of the mean = {error_mc:.5f}")

# Because Stacie can estimate errors of the autocorrelation integral,
# it can also estimate errors of errors of means.
print(f"Uncertainty of the error of mean = {0.5 * result.props['acfint_std'] / error_mc:.5f}")

# %%
# Plot of the empirical and fitted model spectrum.
fig, ax = plt.subplots()
plot_fitted_spectrum(ax, uc, result)

# %%
# Plot of the risk minimization as a function of the frequency cutoff.
fig, ax = plt.subplots()
plot_risk(ax, uc, result)

# %%
# Plot of the error of the mean and its uncertainty as a function of the frequency cutoff.
fig, ax = plt.subplots()
plot_uncertainty(ax, uc, result)

# %% [markdown]
# ## Precise Mean With Numerical Quadrature
#
# Because the probability density is one-dimensional,
# it is feasible to compute the mean using numerical quadrature,
# which is much more accurate than the Monte Carlo estimate.
# (For production simulations, Monte Carlo is only advantageous for high-dimensional problems.)
#
# As shown in the code below, the difference between the quadrature and Monte Carlo estimates
# is on the order of the estimated uncertainty of the MC result.

# %%
numer_quad = quad(lambda r: r * np.exp(logprob(r)), 0, 1000)[0]
denom_quad = quad(lambda r: np.exp(logprob(r)), 0, 1000)[0]
mean_quad = numer_quad / denom_quad
print(f"Quadrature  E[r]   ≈ {mean_quad:8.5f}")
print(f"Monte Carlo E[r]   ≈ {mean_mc:8.5f}")
print(f"|Difference|       = {abs(mean_quad - mean_mc):8.5f}")
print(f"Estimated MC error = {error_mc:8.5f}")
