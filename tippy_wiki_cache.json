{"Thomas%27_cyclically_symmetric_attractor": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Thomas%27_cyclically_symmetric_attractor.png/330px-Thomas%27_cyclically_symmetric_attractor.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In the dynamical systems theory, <b>Thomas' cyclically symmetric attractor</b> is a 3D strange attractor originally proposed by Ren\u00e9 Thomas. It has a simple form which is cyclically symmetric in the x, y, and z variables and can be viewed as the trajectory of a frictionally dampened particle moving in a 3D lattice of forces. The simple form has made it a popular example.</p>", "Discrete-time_Markov_chain": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/330px-Markovkate_01.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability, a <b>discrete-time Markov chain</b> (<b>DTMC</b>) is a sequence of random variables, known as a stochastic process, in which the value of the next variable depends only on the value of the current variable, and not any variables in the past. For instance, a machine may have two states, <i>A</i> and <i>E</i>. When it is in state <i>A</i>, there is a 40% chance of it moving to state <i>E</i> and a 60% chance of it remaining in state <i>A</i>. When it is in state <i>E</i>, there is a 70% chance of it moving to <i>A</i> and a 30% chance of it staying in <i>E</i>. The sequence of states of the machine is a Markov chain. If we denote the chain by <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4e597c7a7aebad9d8017b2accb910f8217ea6bdf\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.671ex;width:14.752ex;height:2.509ex\" /></span> then <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/6381fdad2b9f11954b1fc2db08bbaccf634ededa\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.671ex;width:2.979ex;height:2.509ex\" /></span> is the state which the machine starts in and <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f75f351152fef7494209efa3acdf45be74704196\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.671ex;width:3.8ex;height:2.509ex\" /></span> is the random variable describing its state after 10 transitions. The process continues forever, indexed by the natural numbers.</p>", "Lennard-Jones_potential": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Graph_of_Lennard-Jones_potential.png/330px-Graph_of_Lennard-Jones_potential.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In computational chemistry, molecular physics, and physical chemistry, the <b>Lennard-Jones potential</b> is an intermolecular pair potential. Out of all the intermolecular potentials, the Lennard-Jones potential is probably the one that has been the most extensively studied. It is considered an archetype model for simple yet realistic intermolecular interactions. The Lennard-Jones potential is often used as a building block in molecular models for more complex substances. Many studies of the idealized \"Lennard-Jones substance\" use the potential to understand the physical nature of matter.</p>", "Verlet_integration": "<p><b>Verlet integration</b> is a numerical method used to integrate Newton's equations of motion. It is frequently used to calculate trajectories of particles in molecular dynamics simulations and computer graphics. The algorithm was first used in 1791 by Jean Baptiste Delambre and has been rediscovered many times since then, most recently by Loup Verlet in the 1960s for use in molecular dynamics. It was also used by P. H. Cowell and A. C. C. Crommelin in 1909 to compute the orbit of Halley's Comet, and by Carl St\u00f8rmer in 1907 to study the trajectories of electrical particles in a magnetic field .\nThe Verlet integrator provides good numerical stability, as well as other properties that are important in physical systems such as time reversibility and preservation of the symplectic form on phase space, at no significant additional computational cost over the simple Euler method.</p>", "Nyquist%E2%80%93Shannon_sampling_theorem": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f7/Bandlimited.svg/330px-Bandlimited.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>The <b>Nyquist\u2013Shannon sampling theorem</b> is an essential principle for digital signal processing linking the frequency range of a signal and the sample rate required to avoid a type of distortion called aliasing. The theorem states that the sample rate must be at least twice the bandwidth of the signal to avoid aliasing. In practice, it is used to select band-limiting filters to keep aliasing below an acceptable amount when an analog signal is sampled or when sample rates are changed within a digital signal processing function.</p>", "Mixture_distribution": "<p>In probability and statistics, a <b>mixture distribution</b> is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized. The underlying random variables may be random real numbers, or they may be random vectors, in which case the mixture distribution is a multivariate distribution.</p>", "Digamma_function": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Digamma.png/330px-Digamma.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In mathematics, the <b>digamma function</b> is defined as the logarithmic derivative of the gamma function:</p><dl><dd><span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/49bbff7216b4219800271ad09278195b784bc1a8\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.671ex;width:27.329ex;height:6.509ex\" /></span></dd></dl>", "Gamma_distribution": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Gammapdf252.svg/330px-Gammapdf252.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability theory and statistics, the <b>gamma distribution</b> is a versatile two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution. There are two equivalent parameterizations in common use:</p><ol><li>With a shape parameter <span class=\"texhtml mvar\" style=\"font-style:italic\">\u03b1</span> and a scale parameter <span class=\"texhtml mvar\" style=\"font-style:italic\">\u03b8</span></li>\n<li>With a shape parameter <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b79333175c8b3f0840bfb4ec41b8072c83ea88d3\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.338ex;width:1.488ex;height:1.676ex\" /></span> and a rate parameter <span class=\"nowrap\">\u2060<span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3d9356ec006ad029642da868e5bcac3ef626dd92\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.838ex;width:7.869ex;height:2.843ex\" /></span>\u2060</span></li></ol>", "Trigamma_function": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Psi1.png/330px-Psi1.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In mathematics, the <b>trigamma function</b>, denoted <span class=\"texhtml \"><i>\u03c8</i><sub>1</sub>(<i>z</i>)</span> or <span class=\"texhtml \"><i>\u03c8</i><sup>(1)</sup>(<i>z</i>)</span>, is the second of the polygamma functions, and is defined by</p><dl><dd><span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1929b65abdc95b5ad554f8d9def6b06ea3e66c75\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.171ex;width:19.824ex;height:6.009ex\" /></span>.</dd></dl>", "Spectral_leakage": "<p>The Fourier transform of a function of time, s(t), is a complex-valued function of frequency, S(f), often referred to as a frequency spectrum. Any linear time-invariant operation on s(t) produces a new spectrum of the form H(f)\u2022S(f), which changes the relative magnitudes and/or angles (phase) of the non-zero values of S(f). Any other type of operation creates new frequency components that may be referred to as <b>spectral leakage</b> in the broadest sense. Sampling, for instance, produces leakage, which we call <i>aliases</i> of the original spectral component. For Fourier transform purposes, sampling is modeled as a product between s(t) and a Dirac comb function. The spectrum of a product is the convolution between S(f) and another function, which inevitably creates the new frequency components. But the term 'leakage' usually refers to the effect of <i>windowing</i>, which is the product of s(t) with a different kind of function, the window function. Window functions happen to have finite duration, but that is not necessary to create leakage. Multiplication by a time-variant function is sufficient.</p>", "Standard_score": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/The_Normal_Distribution.svg/330px-The_Normal_Distribution.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In statistics, the <b>standard score</b> or <b><i>z</i>-score</b> is the number of standard deviations by which the value of a raw score is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores.</p>", "List_of_Runge%E2%80%93Kutta_methods": "<p>\n<b>Runge\u2013Kutta methods</b> are methods for the numerical solution of the ordinary differential equation</p><dl><dd><span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c93a137a30865114ceb4da94e5692d32ad783b2b\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-2.005ex;width:13.07ex;height:5.509ex\" /></span></dd></dl>", "Chi-squared_distribution": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/330px-Chi-square_pdf.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability theory and statistics, the <b><span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/8c0cc9237ec72a1da6d18bc8e7fb24cdda43a49a\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.671ex;width:2.509ex;height:3.009ex\" /></span>-distribution</b> with <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.338ex;width:1.211ex;height:2.176ex\" /></span> degrees of freedom is the distribution of a sum of the squares of <span class=\"mwe-math-element mwe-math-element-inline\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c3c9a2c7b599b37105512c5d570edc034056dd40\" class=\"mwe-math-fallback-image-inline mw-invert skin-invert\" aria-hidden=\"true\" style=\"vertical-align:-0.338ex;width:1.211ex;height:2.176ex\" /></span> independent standard normal random variables.</p>", "Periodogram": "<p>In signal processing, a <b>periodogram</b> is an estimate of the spectral density of a signal. The term was coined by Arthur Schuster in 1898. Today, the periodogram is a component of more sophisticated methods. It is the most common tool for examining the amplitude vs frequency characteristics of FIR filters and window functions. FFT spectrum analyzers are also implemented as a time-sequence of periodograms.</p>", "Log-normal_distribution": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/89/Log-normal-pdfs.png/330px-Log-normal-pdfs.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p>In probability theory, a <b>log-normal</b> (or <b>lognormal</b>) <b>distribution</b> is a continuous probability distribution of a random variable whose logarithm is normally distributed. Thus, if the random variable <span class=\"texhtml mvar\" style=\"font-style:italic\">X</span> is log-normally distributed, then <span class=\"texhtml \"><i>Y</i> = ln <i>X</i></span> has a normal distribution. Equivalently, if <span class=\"texhtml mvar\" style=\"font-style:italic\">Y</span> has a normal distribution, then the exponential function of <span class=\"texhtml mvar\" style=\"font-style:italic\">Y</span>, <span class=\"texhtml \"><i>X</i> = exp(<i>Y</i>)</span>, has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values. It is a convenient and useful model for measurements in exact and engineering sciences, as well as medicine, economics and other topics (e.g., energies, concentrations, lengths, prices of financial instruments, and other metrics).</p>", "Chaos_theory": "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Lorenz_attractor_yb.svg/330px-Lorenz_attractor_yb.svg.png\" alt=\"Wikipedia thumbnail\" style=\"float:left; margin-right:10px;\"><p><b>Chaos theory</b> is an interdisciplinary area of scientific study and branch of mathematics. It focuses on underlying patterns and deterministic laws of dynamical systems that are highly sensitive to initial conditions. These were once thought to have completely random states of disorder and irregularities. Chaos theory states that within the apparent randomness of chaotic complex systems, there are underlying patterns, interconnection, constant feedback loops, repetition, self-similarity, fractals and self-organization. The butterfly effect, an underlying principle of chaos, describes how a small change in one state of a deterministic nonlinear system can result in large differences in a later state. A metaphor for this behavior is that a butterfly flapping its wings in Brazil can cause or prevent a tornado in Texas.</p>"}